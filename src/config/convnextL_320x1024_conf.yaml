#TODO: to put relevant description to each element in this yaml file.
model_name: "convnextL_320x1024"

depth_encoder:
  model_type: "convnextL"
  num_layers: 50 # number of ResNet layers used as depth encoder.
  num_features: 256 # ResNet feature dim; basically these is used for decoder and upsampleBN; see the diagram.
  model_dim: 32 # these are basically the number of channels of the resulted feature map S from the ResNet, ConvNeXt-L and Effb5 depth encoder.
  dec_channels: [1024, 512, 256, 128] # decoder channels in Unet for the ConvNeXt-L and EfficientNetB5 depth encoder.

depth_decoder:
  patch_size: 32 # patch size before ViT: "we first apply a convolution of kernel size p x p and stride = p to S"; p=16 in paper!
  dim_out: 64 # hyperparameyter used for bins regressor, where the final Linear Layer of the MLP used this dim_out!
  query_nums: 64 # Q is a hyperparameyter (Q need to satisfy Q<=N=h*w/p^2 ! where (h,w) is the resolution of the output_images feature map S from the
                 # depth encoder) used when we "generate a set of coarse-grained queries of shape R^(CxQ)".
  dim_feedforward: 1024 # this is used as hyperparameter for the TransformerEncoderLayer in depth decoder.
  min_depth: 0.001 # this is the min depth used to calculate the center depths of the bins (see formula (7) from the paper).
  max_depth: 80.0 # this is the max depth used to calculate the center depths of the bins (see formula (7) from the paper).

# misc
# -----------
data_path: '/home/ANT.AMAZON.COM/grigiono/Desktop/SQLdepth/src/data/kitti/kitti_data'
tensorboard_path: '/home/ANT.AMAZON.COM/grigiono/Desktop/SQLdepth/src/tensorboard'
pretrained_models_folder: '/home/ANT.AMAZON.COM/grigiono/Desktop/SQLdepth/src/pretrained/KITTI_ConvNeXt_Large_320x1024_models'
image_path_inference: '/home/ANT.AMAZON.COM/grigiono/Desktop/SQLdepth/src/inference/input_images/img2.png'
                     # I can also give an entire folder with images, not just one image!
                     # choices: ['/home/ANT.AMAZON.COM/grigiono/Desktop/SQLdepth/src/inference/input_images/img2.png',
                     #          '/home/ANT.AMAZON.COM/grigiono/Desktop/SQLdepth/src/inference/input_images']
image_extension_inference: "png"
use_cuda: False
num_workers: 0
im_sz: [320, 1024]
bs: 16

# training
# -----------
training_dataset: "kitti" # dataset to train on; choices=["kitti", "kitti_odom", "kitti_depth", "kitti_test", "cityscapes_preprocessed", "nyu_raw"])
num_epochs: 20 # number of epochs for training.
training_split: "eigen_zhou" # which training split to use;  choices=["eigen_zhou", "eigen_full", "odom", "benchmark", "cityscapes_preprocessed", "nyu_raw"],
learning_rate: 1e-4
scheduler_step_size: 15 # step size of the scheduler used for training.
use_stereo_training: True # if set, uses stereo pair for training.
frame_ids_training: [0, -1, 1] # frames to use for training (current, previous and next); choices: [0, -1, 1], [0, 1]
loss_scales: [0] # scales used in the loss; choices: [0]; [0, 1, 2, 3]
pose_model_input: "pairs" # how many images the pose network gets; choices=["pairs", "all"]
load_pretrained_model: False # if set, uses pretrained encoder and depth decoder for training (both of them!).
load_pretrained_pose: False # if set, uses pretrained PoseNet for training.
train_from_png: True # if set, trains from raw KITTI png files (instead of jpgs).
use_different_learning_rate: False # if set, uses different learning rate for training.
save_frequency: 1 # number of epochs between each save.
log_frequency: 10 # number of batches between each tensorboard log.
use_ssim: True # if set, uses SSIM in the loss.
use_predictive_mask: True # if set, uses a predictive masking scheme as in Zhou et al (in what paper?).
disable_automasking: True # if set, doesn't do auto-masking.
disparity_smoothness_weight: 1e-3
use_average_reprojection: True # if set, uses average reprojection loss.
monodepthv1_multiscale: True # if set, uses Monodepthv1 multiscale.
pose_model_type: posecnn # choices=["posecnn", "pose_flow", "separate_resnet", "shared"] -> what are these options??
models_to_load: ["encoder", "depth", "pose_encoder", "pose"]

# evaluation
# -----------
evaluation_split: "eigen" # choices=["eigen", "eigen_benchmark", "benchmark"]
evaluation_mode: "mono" # choices=["mono", "stereo"]
evaluation_post_process: True # if set will perform the flipping post processing from the original Monodepth paper.
prediction_depth_scale_factor: 1 # if set multiplies predictions by this number.
save_predicted_disparities: False # if set saves predicted disparities.
numpy_disparities_to_evaluate:  # optional path to a .npy disparities file to evaluate.
evaluate_eigen_to_benchmark: False # if set assume we are loading eigen results from .npy but we want to evaluate using the new benchmark.
no_evaluation: False # if set disables evaluation
disable_median_scaling: True # if set disables median scaling in evaluation.